{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "from news_project.dataset import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from news_project.process import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-25 15:22:46.165\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnews_project.dataset\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mAttempting to load data from: ..\\data\\raw\\news.csv\u001b[0m\n",
      "\u001b[32m2025-02-25 15:22:47.449\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnews_project.dataset\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mSuccessfully loaded dataset with 427482 rows and 7 columns.\u001b[0m\n",
      "\u001b[32m2025-02-25 15:22:47.449\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnews_project.dataset\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mColumns in dataset: ['date', 'news', 'neg', 'neu', 'pos', 'compound', 'sentiment']\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>news</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>It was a long antipodean night. While there’s ...</td>\n",
       "      <td>0.059000</td>\n",
       "      <td>0.87800</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>In Mexico there are no licensing or registrati...</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.95600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.296000</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>The government has until Monday to protect the...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.89400</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.381800</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>A record-breaking heat wave in the Southwest i...</td>\n",
       "      <td>0.197505</td>\n",
       "      <td>0.66149</td>\n",
       "      <td>0.141005</td>\n",
       "      <td>0.997491</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>England started its Live Earth concert at Wemb...</td>\n",
       "      <td>0.033000</td>\n",
       "      <td>0.94500</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>-0.177900</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                               news       neg  \\\n",
       "0  2007-07-07  It was a long antipodean night. While there’s ...  0.059000   \n",
       "1  2007-07-07  In Mexico there are no licensing or registrati...  0.044000   \n",
       "2  2007-07-07  The government has until Monday to protect the...  0.000000   \n",
       "3  2007-07-07  A record-breaking heat wave in the Southwest i...  0.197505   \n",
       "4  2007-07-07  England started its Live Earth concert at Wemb...  0.033000   \n",
       "\n",
       "       neu       pos  compound sentiment  \n",
       "0  0.87800  0.064000  0.051600  POSITIVE  \n",
       "1  0.95600  0.000000 -0.296000  NEGATIVE  \n",
       "2  0.89400  0.106000  0.381800  POSITIVE  \n",
       "3  0.66149  0.141005  0.997491  POSITIVE  \n",
       "4  0.94500  0.022000 -0.177900  NEGATIVE  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to the dataset\n",
    "file_path = Path(\"../data/raw/news.csv\")\n",
    "df = load_data(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-25 15:22:47.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnews_project.process\u001b[0m:\u001b[36mremove_duplicates\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mNumber of duplicate rows in 'news' before removal: 12762\u001b[0m\n",
      "\u001b[32m2025-02-25 15:22:47.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnews_project.process\u001b[0m:\u001b[36mremove_duplicates\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mDataset shape after duplicate removal: (414720, 7)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = remove_duplicates(df,\"news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 427482 entries, 0 to 427481\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   date       427482 non-null  object \n",
      " 1   news       427482 non-null  object \n",
      " 2   neg        427482 non-null  float64\n",
      " 3   neu        427482 non-null  float64\n",
      " 4   pos        427482 non-null  float64\n",
      " 5   compound   427482 non-null  float64\n",
      " 6   sentiment  427482 non-null  object \n",
      "dtypes: float64(4), object(3)\n",
      "memory usage: 22.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date         0\n",
      "news         0\n",
      "neg          0\n",
      "neu          0\n",
      "pos          0\n",
      "compound     0\n",
      "sentiment    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "null_counts = df.isnull().sum()\n",
    "print(null_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "num_duplicates = df.duplicated().sum()\n",
    "print(num_duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_col=df.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "categoric_col=df.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date           5892\n",
      "news         414720\n",
      "sentiment         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class_counts = df[categoric_col].nunique()  # Count unique values per column\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg         111977\n",
      "neu         111309\n",
      "pos         111826\n",
      "compound     98818\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class_counts_numeric = df[numeric_col].nunique()  # Count unique values per column\n",
    "print(class_counts_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_6924\\1672415729.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[\"sentiment_encoded\"] = df_cleaned[\"sentiment\"].map(sentiment_mapping)\n"
     ]
    }
   ],
   "source": [
    "sentiment_mapping = {\"NEGATIVE\": 1, \"POSITIVE\": 0}\n",
    "df_cleaned[\"sentiment_encoded\"] = df_cleaned[\"sentiment\"].map(sentiment_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>news</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>It was a long antipodean night. While there’s ...</td>\n",
       "      <td>0.059000</td>\n",
       "      <td>0.87800</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>In Mexico there are no licensing or registrati...</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.95600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.296000</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>The government has until Monday to protect the...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.89400</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.381800</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>A record-breaking heat wave in the Southwest i...</td>\n",
       "      <td>0.197505</td>\n",
       "      <td>0.66149</td>\n",
       "      <td>0.141005</td>\n",
       "      <td>0.997491</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>England started its Live Earth concert at Wemb...</td>\n",
       "      <td>0.033000</td>\n",
       "      <td>0.94500</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>-0.177900</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                               news       neg  \\\n",
       "0  2007-07-07  It was a long antipodean night. While there’s ...  0.059000   \n",
       "1  2007-07-07  In Mexico there are no licensing or registrati...  0.044000   \n",
       "2  2007-07-07  The government has until Monday to protect the...  0.000000   \n",
       "3  2007-07-07  A record-breaking heat wave in the Southwest i...  0.197505   \n",
       "4  2007-07-07  England started its Live Earth concert at Wemb...  0.033000   \n",
       "\n",
       "       neu       pos  compound sentiment  sentiment_encoded  \n",
       "0  0.87800  0.064000  0.051600  POSITIVE                  0  \n",
       "1  0.95600  0.000000 -0.296000  NEGATIVE                  1  \n",
       "2  0.89400  0.106000  0.381800  POSITIVE                  0  \n",
       "3  0.66149  0.141005  0.997491  POSITIVE                  0  \n",
       "4  0.94500  0.022000 -0.177900  NEGATIVE                  1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess_text\u001b[39m(text):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vectorizer\u001b[38;5;241m.\u001b[39mtransform([text])\n\u001b[1;32m---> 17\u001b[0m news_tfidf_parallel \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_text\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf_cleaned\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnews\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Convert the parallel-processed sparse matrix into a single matrix\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\qafza_task\\lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\qafza_task\\lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\qafza_task\\lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# ✅ 1. Initialize & Fit TF-IDF Vectorizer (Remove n_jobs)\n",
    "vectorizer = TfidfVectorizer(max_features=20000)\n",
    "\n",
    "# ✅ 2. Fit TF-IDF only on the \"news\" column\n",
    "vectorizer.fit(df_cleaned[\"news\"])\n",
    "\n",
    "# ✅ 3. Parallel Processing (Transform Large Dataset Faster)\n",
    "def process_text(text):\n",
    "    return vectorizer.transform([text])\n",
    "\n",
    "news_tfidf_parallel = Parallel(n_jobs=-1)(delayed(process_text)(text) for text in df_cleaned[\"news\"])\n",
    "\n",
    "# Convert the parallel-processed sparse matrix into a single matrix\n",
    "import scipy.sparse\n",
    "news_tfidf = scipy.sparse.vstack(news_tfidf_parallel)  # Stack transformed rows\n",
    "\n",
    "# ✅ 4. Convert to DataFrame\n",
    "df_tfidf = pd.DataFrame(news_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# ✅ 5. Add the Target Column\n",
    "df_tfidf[\"sentiment_encoded\"] = df_cleaned[\"sentiment_encoded\"]\n",
    " \n",
    "# ✅ 6. Compute Correlation\n",
    "correlation = df_tfidf.corr()[\"sentiment_encoded\"].sort_values(ascending=False)\n",
    "\n",
    "# ✅ 7. Print Results\n",
    "print(\"Top correlated words with sentiment:\")\n",
    "print(correlation.head(10))  # Show top 10 correlated words\n",
    "\n",
    "# ✅ 8. Display TF-IDF Transformed Data\n",
    "print(df_tfidf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# ✅ Save the TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, \"news_project/models/vectorizer_models/tfidf_vectorizer.pkl\")\n",
    "print(\"TF-IDF Vectorizer saved successfully!\")\n",
    "# ✅ Save the LabelEncoder\n",
    "\n",
    "# ✅ Save the TF-IDF Transformed Data for future use\n",
    "df_tfidf.to_csv(\"data/interim/tfidf_transformed_data.csv\", index=False)\n",
    "print(\"TF-IDF transformed data saved successfully!\")\n",
    "\n",
    "# ✅ Save the transformed sparse matrix (alternative to CSV for large datasets)\n",
    "joblib.dump(news_tfidf, \"news_project/models/tfidf_transformed/tfidf_transformed_matrix.pkl\")\n",
    "print(\"Sparse TF-IDF matrix saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_tfidf.drop([\"sentiment_encoded\"], axis=1)\n",
    "target = df_tfidf[\"sentiment_encoded\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from news_project.modeling.train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df = pd.DataFrame()\n",
    "trained_model = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different models\n",
    "models_df,trained_models = train_and_evaluate(LinearRegression(), X_train, y_train, X_test, y_test, models_df,trained_model)\n",
    "models_df,trained_models = train_and_evaluate(Ridge(alpha=1.0), X_train, y_train, X_test, y_test, models_df,trained_model)\n",
    "models_df,trained_models = train_and_evaluate(RandomForestRegressor(n_estimators=100), X_train, y_train, X_test, y_test, models_df,trained_model)\n",
    "models_df,trained_models = train_and_evaluate(ElasticNet(alpha=1.0, l1_ratio=0.5), X_train, y_train, X_test, y_test, models_df,trained_model)\n",
    "models_df,trained_models = train_and_evaluate(Lasso(alpha=1.0), X_train, y_train, X_test, y_test, models_df,trained_model)\n",
    "models_df,trained_models = train_and_evaluate(SVR(kernel=\"rbf\", C=1.0, epsilon=0.1), X_train, y_train, X_test, y_test, models_df,trained_model)\n",
    "# models_df,trained_models = train_and_evaluate(XGBRegressor(n_estimators=100, learning_rate=0.1), X_train, y_train, X_test, y_test, models_df,trained_model)\n",
    "display(models_df)  # Shows the DataFrame in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from news_project.process import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_news_text(text):\n",
    "    text = remove_byte_prefix(text)\n",
    "    text = normalize_quotes(text)\n",
    "    text = remove_special_chars(text)\n",
    "    text = remove_urls(text)\n",
    "    text = to_lowercase(text)\n",
    "    # Finally, tokenize + remove stopwords + lemmatize\n",
    "    text = tokenize_lemmatize(text)\n",
    "    return text\n",
    "df_cleaned[\"cleaned_news\"] = df_cleaned[\"news\"].apply(clean_news_text)\n",
    "df_cleaned = df_cleaned[df_cleaned[\"cleaned_news\"].str.strip().astype(bool)]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qafza_task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
