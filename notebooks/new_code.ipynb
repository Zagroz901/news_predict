{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-25 18:20:21.179\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnews_project.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: C:\\Users\\Lenovo\\Desktop\\Train_Qafza\\Project\\news_project_pipline\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "from news_project.dataset import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from news_project.process import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-25 18:20:22.559\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnews_project.dataset\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mAttempting to load data from: ..\\data\\raw\\news.csv\u001b[0m\n",
      "\u001b[32m2025-02-25 18:20:23.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnews_project.dataset\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mSuccessfully loaded dataset with 427482 rows and 7 columns.\u001b[0m\n",
      "\u001b[32m2025-02-25 18:20:23.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnews_project.dataset\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mColumns in dataset: ['date', 'news', 'neg', 'neu', 'pos', 'compound', 'sentiment']\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>news</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>It was a long antipodean night. While there’s ...</td>\n",
       "      <td>0.059000</td>\n",
       "      <td>0.87800</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>In Mexico there are no licensing or registrati...</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.95600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.296000</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>The government has until Monday to protect the...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.89400</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.381800</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>A record-breaking heat wave in the Southwest i...</td>\n",
       "      <td>0.197505</td>\n",
       "      <td>0.66149</td>\n",
       "      <td>0.141005</td>\n",
       "      <td>0.997491</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>England started its Live Earth concert at Wemb...</td>\n",
       "      <td>0.033000</td>\n",
       "      <td>0.94500</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>-0.177900</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                               news       neg  \\\n",
       "0  2007-07-07  It was a long antipodean night. While there’s ...  0.059000   \n",
       "1  2007-07-07  In Mexico there are no licensing or registrati...  0.044000   \n",
       "2  2007-07-07  The government has until Monday to protect the...  0.000000   \n",
       "3  2007-07-07  A record-breaking heat wave in the Southwest i...  0.197505   \n",
       "4  2007-07-07  England started its Live Earth concert at Wemb...  0.033000   \n",
       "\n",
       "       neu       pos  compound sentiment  \n",
       "0  0.87800  0.064000  0.051600  POSITIVE  \n",
       "1  0.95600  0.000000 -0.296000  NEGATIVE  \n",
       "2  0.89400  0.106000  0.381800  POSITIVE  \n",
       "3  0.66149  0.141005  0.997491  POSITIVE  \n",
       "4  0.94500  0.022000 -0.177900  NEGATIVE  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to the dataset\n",
    "file_path = Path(\"../data/raw/news.csv\")\n",
    "df = load_data(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-25 18:20:24.598\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnews_project.process\u001b[0m:\u001b[36mremove_duplicates\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mNumber of duplicate rows in 'news' before removal: 12762\u001b[0m\n",
      "\u001b[32m2025-02-25 18:20:24.681\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnews_project.process\u001b[0m:\u001b[36mremove_duplicates\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mDataset shape after duplicate removal: (414720, 7)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = remove_duplicates(df,\"news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_9372\\2016464974.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[\"cleaned_news\"] = df_cleaned[\"news\"].apply(clean_news_text)\n"
     ]
    }
   ],
   "source": [
    "def clean_news_text(text):\n",
    "    text = remove_byte_prefix(text)\n",
    "    text = normalize_quotes(text)\n",
    "    text = remove_special_chars(text)\n",
    "    text = remove_urls(text)\n",
    "    text = to_lowercase(text)\n",
    "    # Finally, tokenize + remove stopwords + lemmatize\n",
    "    text = tokenize_lemmatize(text)\n",
    "    return text\n",
    "df_cleaned[\"cleaned_news\"] = df_cleaned[\"news\"].apply(clean_news_text)\n",
    "df_cleaned = df_cleaned[df_cleaned[\"cleaned_news\"].str.strip().astype(bool)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was a long antipodean night. While there’s no telling what’s a reflection of national taste and what’s the result of booking expediency, each of the concerts from Australia, Japan and China certainly had its own character.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned[\"news\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'long antipodean night telling reflection national taste result booking expediency concert australia japan china certainly character'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned[\"cleaned_news\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "print(nltk.data.find('tokenizers/punkt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "print(nltk.data.find('tokenizers/punkt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_6924\\1672415729.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[\"sentiment_encoded\"] = df_cleaned[\"sentiment\"].map(sentiment_mapping)\n"
     ]
    }
   ],
   "source": [
    "sentiment_mapping = {\"NEGATIVE\": 1, \"POSITIVE\": 0}\n",
    "df_cleaned[\"sentiment_encoded\"] = df_cleaned[\"sentiment\"].map(sentiment_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>news</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>It was a long antipodean night. While there’s ...</td>\n",
       "      <td>0.059000</td>\n",
       "      <td>0.87800</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>In Mexico there are no licensing or registrati...</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.95600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.296000</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>The government has until Monday to protect the...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.89400</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.381800</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>A record-breaking heat wave in the Southwest i...</td>\n",
       "      <td>0.197505</td>\n",
       "      <td>0.66149</td>\n",
       "      <td>0.141005</td>\n",
       "      <td>0.997491</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>England started its Live Earth concert at Wemb...</td>\n",
       "      <td>0.033000</td>\n",
       "      <td>0.94500</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>-0.177900</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                               news       neg  \\\n",
       "0  2007-07-07  It was a long antipodean night. While there’s ...  0.059000   \n",
       "1  2007-07-07  In Mexico there are no licensing or registrati...  0.044000   \n",
       "2  2007-07-07  The government has until Monday to protect the...  0.000000   \n",
       "3  2007-07-07  A record-breaking heat wave in the Southwest i...  0.197505   \n",
       "4  2007-07-07  England started its Live Earth concert at Wemb...  0.033000   \n",
       "\n",
       "       neu       pos  compound sentiment  sentiment_encoded  \n",
       "0  0.87800  0.064000  0.051600  POSITIVE                  0  \n",
       "1  0.95600  0.000000 -0.296000  NEGATIVE                  1  \n",
       "2  0.89400  0.106000  0.381800  POSITIVE                  0  \n",
       "3  0.66149  0.141005  0.997491  POSITIVE                  0  \n",
       "4  0.94500  0.022000 -0.177900  NEGATIVE                  1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# ✅ 1. Initialize & Fit TF-IDF Vectorizer (Remove n_jobs)\n",
    "vectorizer = TfidfVectorizer(max_features=20000)\n",
    "\n",
    "# ✅ 2. Fit TF-IDF only on the \"news\" column\n",
    "vectorizer.fit(df_cleaned[\"news\"])\n",
    "\n",
    "# ✅ 3. Parallel Processing (Transform Large Dataset Faster)\n",
    "def process_text(text):\n",
    "    return vectorizer.transform([text])\n",
    "\n",
    "news_tfidf_parallel = Parallel(n_jobs=-1)(delayed(process_text)(text) for text in df_cleaned[\"news\"])\n",
    "\n",
    "# Convert the parallel-processed sparse matrix into a single matrix\n",
    "import scipy.sparse\n",
    "news_tfidf = scipy.sparse.vstack(news_tfidf_parallel)  # Stack transformed rows\n",
    "\n",
    "# ✅ 4. Convert to DataFrame\n",
    "df_tfidf = pd.DataFrame(news_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# ✅ 5. Add the Target Column\n",
    "df_tfidf[\"sentiment_encoded\"] = df_cleaned[\"sentiment_encoded\"]\n",
    " \n",
    "# ✅ 6. Compute Correlation\n",
    "correlation = df_tfidf.corr()[\"sentiment_encoded\"].sort_values(ascending=False)\n",
    "\n",
    "# ✅ 7. Print Results\n",
    "print(\"Top correlated words with sentiment:\")\n",
    "print(correlation.head(10))  # Show top 10 correlated words\n",
    "\n",
    "# ✅ 8. Display TF-IDF Transformed Data\n",
    "print(df_tfidf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# ✅ Save the TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, \"news_project/models/vectorizer_models/tfidf_vectorizer.pkl\")\n",
    "print(\"TF-IDF Vectorizer saved successfully!\")\n",
    "# ✅ Save the LabelEncoder\n",
    "\n",
    "# ✅ Save the TF-IDF Transformed Data for future use\n",
    "df_tfidf.to_csv(\"data/interim/tfidf_transformed_data.csv\", index=False)\n",
    "print(\"TF-IDF transformed data saved successfully!\")\n",
    "\n",
    "# ✅ Save the transformed sparse matrix (alternative to CSV for large datasets)\n",
    "joblib.dump(news_tfidf, \"news_project/models/tfidf_transformed/tfidf_transformed_matrix.pkl\")\n",
    "print(\"Sparse TF-IDF matrix saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_tfidf.drop([\"sentiment_encoded\"], axis=1)\n",
    "target = df_tfidf[\"sentiment_encoded\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from news_project.modeling.train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df = pd.DataFrame()\n",
    "trained_model = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different models\n",
    "models_df,trained_models = train_and_evaluate(LinearRegression(), X_train, y_train, X_test, y_test, models_df,trained_model)\n",
    "models_df,trained_models = train_and_evaluate(Ridge(alpha=1.0), X_train, y_train, X_test, y_test, models_df,trained_model)\n",
    "models_df,trained_models = train_and_evaluate(RandomForestRegressor(n_estimators=100), X_train, y_train, X_test, y_test, models_df,trained_model)\n",
    "models_df,trained_models = train_and_evaluate(ElasticNet(alpha=1.0, l1_ratio=0.5), X_train, y_train, X_test, y_test, models_df,trained_model)\n",
    "models_df,trained_models = train_and_evaluate(Lasso(alpha=1.0), X_train, y_train, X_test, y_test, models_df,trained_model)\n",
    "models_df,trained_models = train_and_evaluate(SVR(kernel=\"rbf\", C=1.0, epsilon=0.1), X_train, y_train, X_test, y_test, models_df,trained_model)\n",
    "# models_df,trained_models = train_and_evaluate(XGBRegressor(n_estimators=100, learning_rate=0.1), X_train, y_train, X_test, y_test, models_df,trained_model)\n",
    "display(models_df)  # Shows the DataFrame in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from news_project.process import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_news_text(text):\n",
    "    text = remove_byte_prefix(text)\n",
    "    text = normalize_quotes(text)\n",
    "    text = remove_special_chars(text)\n",
    "    text = remove_urls(text)\n",
    "    text = to_lowercase(text)\n",
    "    # Finally, tokenize + remove stopwords + lemmatize\n",
    "    text = tokenize_lemmatize(text)\n",
    "    return text\n",
    "df_cleaned[\"cleaned_news\"] = df_cleaned[\"news\"].apply(clean_news_text)\n",
    "df_cleaned = df_cleaned[df_cleaned[\"cleaned_news\"].str.strip().astype(bool)]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qafza_task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
